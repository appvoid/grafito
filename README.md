# grafito: Great Retreiver Alignment For Intelligence Through Open-domain
![Alt Text](https://raw.githubusercontent.com/appvoid/grafito/1a5d76f45bb1ceb5ca012b6337072796df00c383/grafito.gif)
This is a public research experiment and personal project on a of prompt-fine-tuned GPT models for generation tasks on chat form. Each one of these iterations are in current evaluation (prompt-engineering) to explore better ways to extract useful conversational tasks from them. An Android app is on the way. Link will be available through this file in the future. "Grafito" comes from Spanish word and it means Graphite.

[ðŸ‘‰ Google Play Store App link](https://play.google.com/store/apps/details?id=com.nohakcoffeeofficial.grafitoai)

Note that finetuned-codename-version and codename-version are used interchangebly here and that the current models, names and/or techniques are prone to change over time.

The techniques used are One-Shot and Few-Shot learning. Future iterations will adopt a mixture of Few-Shot and MultiModal Intent Classification.

### Models in current evaluation
| Codename    |Official name| Parameters  |     Learning Style     | Published|
| ----------- | ----------- | ----------- | ---------------------- | -------- |
| Diamond-001 | GPT-J       | 6 Billion   | Fine-Tunning (FS)      | â¬›       |
| Diamond-002 | Curie       | 6.7 Billion | Fine-Tunning (MMIC)    | â¬›       |
| Diamond-003 | Davinci     | 175 Billion | Fine-Tunning (FS)      | ðŸŸ©       |
| Diamond-004 | LLaMA       | 6.7 Billion | Fine-Tunning (MMIC)    | â¬›       |


Once Alpaca models are released, [see here](https://crfm.stanford.edu/2023/03/13/alpaca.html), an even more customized, better fine-tuning will be made and a reward model will be included, GPT-4 level you would say. Still waiting for LLaMA license change to be approved. Something to note here is that these models cannot be used (yet) as commercial products.

### Deprication note
Open Source language models were depricated to focus more on the final, usable product itself. The technique used on this project works pretty good on GPT-J though.

### Deprication note #2
Open Source language models are (again, thanks to Meta) back into consideration.
